\chapter{LITERATURE REVIEW}

\section{Introduction}
The technology for Automatic Speech Recognition (ASR) has advanced rapidly in these years. Early traditional models like GMM and HMM into more sophisticated deep learning approaches such as DNN, CNN, RNN, and Transformer-based architectures. 

\begin{figure}[H]
    \centering
    \includegraphics[width=.75\textwidth]{mainmatter//images/mindmap.png}
    \caption{Literature Review Mind Map}
\end{figure}

However, to apply these technologies to the Japanese language may pose few challenges due to its complex writing systems. In this chapter, the challenges of Japanese speech detection and the traditional and modern ASR models will be reviewed. The state-of-the-art model like Whisper, wav2vec, and Chirp will also be discussed based on their applicability to Japanese. By identifying the key challenges and gaps in existing research, this chapter prepared for a focused analysis of Japanese-specific ASR systems.

\section{Challenges in Japanese Speech Detection}
\subsection{Complexity of Japanese Writing System and Characters}
The complexity of Japanese writing system and character can cause challenge in ASR system especially in end-to-end neural network architectures. Japanese writing system is a combination of multiple character sets, such as the Hiragana, Katakana, Kanji (ideographic characters), Roman letters and various symbols, leading to a considerably larger and more varied character \parencite{rose2019unique}. As mentioned by \textcite{Ito2016End-to-end,ito2017}, the number of possible Japanese character labels can exceed several thousand.

A single character of kanji may have a few ways to pronounce it because each character of kanji has Onyomi (Chinese derived) and Kunyomi (native Japanese) readings, and these readings can change depending on the word context \parencite{curtin2020japanese}. Because of this ambiguity, the ASR system must be able to model and distinguish numerous acoustic differences in the speech data with same sound. The training model must be able to handle thousands of character and each of the character is potentially linked to multiple context dependent phonetic outcome which require a significant computational resources and large scale training data to ensure adequate coverage \parencite{Ito2016End-to-end,ito2017}.


\section{Traditional Speech Detection Models } 

\subsection{Gaussian Mixture Models (GMM)}
GMM have been the earliest technology used for developing Japanese speech detection and recognition systems because of their capability in capturing the statistical distribution of speech features very well \parencite{Imaishi2022Examination}. Because of the absence of word boundaries and the nuances of pitch accent in the Japanese language, it is really complicated to understand the context of the spoken words. However, GMM would be useful by using probabilities to manage and characterize intricate patterns \parencite{sun2020subspace}. For example, \textcite{povey2011subspace} were able to use GMM to model phoneme-based acoustic features, and this approach led to a good performance of speech recognition systems.

 \textcite{Imaishi2022Examination} developed an approach within the EM algorithm that leads to the stabilization of the GMM parameters as well as increasing the discriminative power of the model in cases where there is not much evaluation data available. In other work, \textcite{povey2011subspace} point out that it is possible to represent the distribution of speech features in GMM mode by employing a combination of several Gaussian components. \textcite{Takami2020Performance} emphasized a different direction which starts with the creation of the Universal Background Model, which is a Gaussian Mixture Model calculated from the collection of a large number of speech samples. 

\subsection{Hidden Markov Models (HMM)}
HMM is working quite well with Japanese speech detection because of the incorporation of the acoustic and temporal characteristics of speech, including the difficulties found in the encoding of Japanese speech \parencite{Tokuda1999Application}. ASR systems incorporated with HMM are more superior in portraying Japanese speech characteristics’ rhythm and tone including essential features like pitch accent and moraic timing which features will enhance the performance of the systems on the phonology aspects of the language \parencite{Tokuda2000HMM}.

To further Increase Japanese ASR capability, few other models is used along with HMM which is context-sensitive such as Tri-phone method. Tri-phone method is a phonetic expansion that employs phonetics of the neighbor sounds to the phoneme as context in order to increase the recognition accuracy by taking into account the co-articulation that takes place during fast speech production \parencite{Tokuda2000HMM}. 


\subsection{The GMM-HMM Combination}
In the Kaldi framework, Japanese large-vocabulary ASR has commonly been implemented as a GMM--HMM pipeline where HMM states capture temporal dynamics and GMMs model frame-level acoustic likelihoods, typically paired with context-dependent decision-tree clustered triphones. Classic Kaldi recipes also report stepwise gains when moving from simpler triphone systems to stronger feature-space modeling. For example, the Kaldi CSJ recipe reports that performance improves from early triphone stages to later stages, with eval1 WER decreasing from 22.67\% (tri1) to 21.49\% (tri2) and further to 17.49\% (tri3) and 15.26\% (tri4), demonstrating the typical benefit of successive refinements in the conventional HMM-based pipeline. \parencite{KaldiCSJResults2015}

More recently, \textcite{ando2021} evaluated Kaldi-style HMM-based systems using the official CSJ recipe as a baseline training procedure and reported CER on both an out-of-domain TEDx-style test set and the standard CSJ evaluation sets. On the TEDxJP-10K evaluation set, their best reported configuration achieved 17.92\% CER, while simpler configurations such as CSJ acoustic model with CSJ language model achieved 23.41\% CER. These results provide concrete reference points for conventional HMM-based Japanese ASR performance under both matched (CSJ) and more realistic, diverse conditions (TEDxJP-10K). \parencite{ando2021}

\begin{table}[H]
\centering
\caption{ASR decoding results (CER\%) on the TEDxJP-10K dataset (adapted from \textcite{ando2021}).}
\label{tab:tedxjp10k_cer}
{\small
\setlength{\tabcolsep}{8pt}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lccc}
\hline
\textbf{LM} & \textbf{CSJ} & \textbf{TV} & \textbf{CSJ+TV} \\
\hline
CSJ        & 23.41 & 21.83 & 20.61 \\
TV         & 22.14 & 18.98 & 18.22 \\
OSCAR      & 23.52 & 19.28 & 18.71 \\
TV+OSCAR   & 21.89 & 18.29 & 17.92 \\
\hline
\end{tabular}}
\end{table}



\section{Modern Deep Learning Approaches}
\subsection{Deep Neural Networks (DNN)}
The use of DNN in conjunction with HMM, also known as DNN-HMM has been shown to improve performance in Japanese ASR. \textcite{seki2014comparison} compared syllable-based and phoneme-based DNN-HMM and found that the syllable-based DNN-HMM was better, as its parameter space is less coupled with the context of the syllables. They reported that an 11\% relative decrease in the WER for triphone DNN-HMMs over syllable-based DNN-HMMs when used on large databases such as ASJ+JNAS. The multilayered structure of DNNs makes it much suitable for developing models of contextual dependencies for speech signals \parencite{hojo2018dnn}. 

\textcite{mu2020japanese} developed a double-deep neural network for the evaluation of Japanese pronunciation to address the problems of text-to-speech alignment and scoring. The DDNN integrated CNN and RNNs with attention and it is effective for detecting pronunciation mistakes. \textcite{lin2017dnn} noted the importance of addressing the particular problem of the lack of annotated Japanese speech corpora by emphasizing the use of transfer learning with DNN. 

\subsection{Convolutional Neural Networks (CNN)}
\parencite{noda2014lipreading} conducted a research using elastic net regression on a seven-layer CNN structure and 58\% of phoneme recognition accuracy was obtained for Japanese datasets. Building upon this work, \textcite{yalta2019cnn} constructed a functional speech recognition framework inclusive of several types of words spoken intended for tight spots like houses. 

\textcite{noda2014lipreading} investigated the use of CNNs for solving the problem of creating a Japanese speech acoustic model. CNN used to encode the frequency-time domain audio and properly exploit the spatial and temporal aspects. The combination of CNNs with attention mechanisms has been beneficial in increasing accuracy and interoperability during the detection of long utterances and multi-speaker datasets \parencite{Mukohara2015Emotion}.


\subsection{Recurrent Neural Networks (RNN)}
In the work of \textcite{takeuchi2020real}, RNN is introduced, which enables the processing of input speech while removing noise and help to reduce exploding gradient problems often seen in RNNs. \textcite{Kida2016LSTM} investigated linear prediction filters based on LSTM which did not require direct access to raw information and thus can extract features from distorted signals, as an LSTM estimated linear prediction coefficients. 

\textcite{Kubo2014DeepLearning} broadened approaches incorporating RNNs into synthesizing speech for Japanese, focusing on improving prosody and intonation. \textcite{takeuchi2020real} took advantage of the RNN-based architectures for the acoustic modelling for Japanese ASR showed that even though GRUs have a simpler gating strategy than LSTMs, they could achieve a similar level of classification accuracy with lower compute requirements. Then, the studies on bidirectional LSTMs (BLSTMs) by \textcite{imaizumi2022} revealed that the past context and the future context of the signal can be utilized for better performance of ASR. 




\subsection{Convolutional-Recurrent DNN with Connectionist Temporal Classification (CRDNN-CTC)}
The CRDNN-CTC family is part of the broader end-to-end ASR direction that replaces the conventional lexicon-driven alignment pipeline with direct sequence learning. In this approach, acoustic features are mapped to output symbols using a neural encoder, and the Connectionist Temporal Classification (CTC) objective provides a monotonic alignment between the input frames and the target transcription without requiring frame-level labels. This has been used in Japanese ASR because Japanese can be transcribed naturally at the character or kana level, while word boundaries are not explicitly marked in continuous speech, making forced alignment and word-level tokenization less straightforward \parencite{watanabe2017hybrid, watanabe2018espnetendtoendspeechprocessing}.

A consistent finding in prior work is that CTC-based models can achieve strong Japanese recognition accuracy while maintaining a simple training pipeline. For example, \textcite{chen2020streaming} proposed an end-to-end streaming Japanese speech recognition method that combines CTC with local attention and reported a best CER of 9.87\%, demonstrating that competitive character-level performance is possible even under streaming constraints where future context is limited. This line of work also supports the practicality of CTC-style training for Japanese, where stable alignments can be learned directly from paired audio-transcript data without the needs of pronunciation dictionaries \parencite{chen2020streaming}.

\begin{table}[H]
\centering
\caption{Streaming Japanese ASR results (Avg. CER\% and latency) using CTC and local attention (from \textcite{chen2020streaming}).}
\label{tab:chen2020_streaming_avgcer_latency}
{\small
\setlength{\tabcolsep}{7pt}
\renewcommand{\arraystretch}{1.15}
\begin{tabular}{c c c c r}
\hline
\textbf{Model no.} & \textbf{CNN} & \textbf{Attn.} & \textbf{Latency (ms)} & \textbf{Avg. CER (\%)} \\
\hline
1 &  &  &  40  & 12.60 \\
2 &  &  &  60  & 13.67 \\
3 &  & $\checkmark$ & 240  & 10.37 \\
4 &  & $\checkmark$ & 360  & 10.40 \\
5 & $\checkmark$ &  &  40  & 10.03 \\
6 & $\checkmark$ &  &  60  & 12.27 \\
7 & $\checkmark$ & $\checkmark$ & 240  &  9.87 \\
8 & $\checkmark$ & $\checkmark$ & 360  & 10.27 \\
9 & $\checkmark$ & $\checkmark$ & 360  & 10.23 \\
\hline
\end{tabular}}
\end{table}

\section{Transformers Models in Japanese Speech Recognition}
\subsection{Transformer-based Models}  
\textcite{taniguchi2022transformer} propose a series of Transformer-based ASR models aimed at improving Japanese speech recognition, particularly in the context of simultaneous interpretation. They investigate the possibility of utilizing auxiliary input like the source language text to resolve issues such as disfluencies, hesitations, and self-repairs commonly observed in the interpreter speech which helps to improve the transcription quality \parencite{Futami2020Bidirectional}. The models combined audio and text data via multi-modal transformer encoders and decoders, which offers a broader scope of recognition by using previously provided source language text for interpreter training programs \parencite{taniguchi2022transformer}.

A wide range of datasets for source text and simultaneous interpretation speech are however not readily available, so the authors use a adapted speech translation corpora from MuST-C and CoVoST 2 while also introducing TED based Japanese texts for evaluation purposes \parencite{taniguchi2022transformer}. With an additional goal of enhancing performance, the authors fine-tune the source language text encoder by using large machine translation corpora which helps in lowering the word error rates during translation of English, Dutch, German and Japanese \parencite{Taniguchi2024Pretraining}. Results consistently demonstrate that incorporating source language text into Transformer-based ASR models significantly improves recognition performance, with the greatest impact observed when auxiliary input is introduced at later stages of the audio encoding and decoding process \parencite{Futami2020Bidirectional}.

\subsection{Whisper by OpenAI}
Large scale weak supervision has emerged as one of the major approaches in speech recognition as noted by \textcite{radford2023robust} in their development of whisper model that has been trained on multilingual and multitask audio datasets that has a combined duration of 680,000 hours. This work is a continuation to the self-supervised methods such as Wav2Vec 2.0 \parencite{baevski2020wav2vec}, which demonstrated learning without supervision from audio without any human-provided labels. However, dataset-specific fine-tuning is often necessary to obtain good performance, whereas with Whisper such reliance is reduced because of the efficacy of weak supervision. 

By scaling weak supervision across diverse datasets, Whisper able to bypass the need for dataset-specific adaptation while able offer a robust zero-shot performance across languages and tasks. This method also resulting in the models to have similar trends with other state of the art model in machine learning where a large, diverse datasets will improve model resilience which is align the with the advancements in computer vision \parencite{kolesnikov2020big} and NLP \parencite{radford2019language}.The Whisper model’s architecture, a simple encoder-decoder Transformer, reinforces the effectiveness of minimal preprocessing and sequence-to-sequence training, simplifying the transcription pipeline.

\textcite{bajo2024efficient} detail their work on adapting OpenAI’s Whisper model to enhance its performance in ASR for the Japanese language. The research draws attention to the dilemma faced in balancing the multilingual being and the accuracy of an English-only product, ReazonSpeech, that seeks to maximize on the Japanese language ASR, which is monolingual in nature. By using a Japanese dataset while utilizing Low-Rank Adaptation (LoRA) and fine-tuning methods, they were able to lower Whisper-Tiny’s CER from 32.7\% to 14.7\%. This fine tuning method showed that smaller multilingual models give more promising result, after being tuned for the desired language outperform their larger baseline models, for example the case of the Whisper-Base model \parencite{bajo2024efficient}.


\begin{table}[ht]
    \centering
    \caption{WER and CER performance of Whisper models. Reproduced from \cite{bajo2024efficient}.}
    \begin{tabular}{p{4cm}|p{3cm}|p{3cm}}
    \hline
    \textbf{Model}        & \textbf{WER (\%)} & \textbf{CER (\%)} \\ \hline
    Whisper Tiny          & 47.48             & 32.74             \\ 
    Whisper Base          & 29.81             & 20.20             \\ 
    Whisper Small         & 16.14             & 9.89              \\ 
    Whisper Medium        & 10.84             & 6.86              \\ 
    Whisper Large         & 7.41              & 4.77              \\ \hline
    Whisper Tiny + LoRA   & 33.16             & 20.83             \\ 
    Whisper Base + LoRA   & 23.36             & 14.50             \\ 
    Whisper Small + LoRA  & 14.90             & 9.16              \\ \hline


    \end{tabular}
    \label{tab:whisper-performance}
    \end{table}
    
  
\section{Current Comparative Analysis of Japanese ASR Models}
A comparative analysis that carried out by \textcite{Karita2021} shows that Conformer-based models perform better than Conformer BLSTM architectures, as they obtained 4.1, 3.2, and 3.5 character error rates for CSJ in eval1, eval2, and eval3 tasks respectively. It is noted that both the BLSTM and Conformer models have character error rates below 7\% and the character error rate is lower when using Conformer Itself. Conformer encoders also offer increased accuracy and efficiency, with a throughput of 628.4 utterances processed per second and 430.0 for the BLSTM models \parencite{Karita2021}.

\begin{table}[h!]
    \centering
    \caption{Character error rates on CSJ dev/eval1/eval2/eval3 sets cited from \cite{Karita2021}.}
    \begin{tabular}{l|l|c|c|c}
    \hline
    \textbf{Encoder}   & \textbf{Decoder}   & \textbf{Param} & \textbf{Utt/sec} & \textbf{CER [\%]} \\ \hline
    BLSTM              & CTC               & 258M             & 430.0            & 3.9 / 5.2 / 3.7 / 4.0 \\ 
    BLSTM              & attention         & 309M             & 365.5            & 3.8 / 5.3 / 3.7 / 3.7 \\ 
    BLSTM              & transducer        & 274M             & 297.6            & 3.8 / 5.1 / 3.7 / 4.0 \\ 
    Conformer          & CTC               & 117M             & \textbf{628.4}   & 3.1 / 4.1 / 3.2 / 3.5 \\ 
    Conformer          & attention         & 124M             & 534.8            & 3.3 / 4.5 / 3.3 / 3.5 \\ 
    Conformer          & transducer        & 120M             & 376.1            & \textbf{3.1 / 4.1 / 3.2 / 3.5} \\ \hline
    \end{tabular}
    \label{tab:cer_comparison}
    \end{table}

Another comparative analysis in ASR for Japanese is presented by \textcite{takahashi2024comparison}, focusing on the accuracy of speech recognition for different dialects. The study evaluates three models: Whisper, XLSR, and XLS-R, which are self-supervised learning frameworks. The Whisper model significantly underperformed for any Japanese outside of standard Japanese, recording a 4.1\% CER only after it has gone through fine tuning. However, when the accuracy is low when the language identification marker is absent where some instances of being higher than 100\%.This marks the weakness of Whisper in terms of its application for wide ranging applications in different dialects of Japanese \parencite{takahashi2024comparison}.


\begin{table}[H]
    \centering
    \caption{Comparison of ASR accuracy on two datasets, Standard Japanese (CSJ) and Japanese dialects (COJADS) cited from \cite{takahashi2024comparison}.}
    \begin{tabular}{c|c|c|c}
    % \begin{tabular}{p{2.7cm}|c|p{2cm}|p{2cm}}

    \hline
    \textbf{Pre-Trained Model Name} & \textbf{Adaptation Method} & \textbf{CER [\%] CSJ} & \textbf{CER [\%] COJADS} \\ \hline
    Whisper-medium (zeroshot)       & -                              & 25.6*                 & 116.0*                   \\ \hline
    Whisper-medium                  & full finetuning                & \textbf{4.1}          & 32.9                     \\ \hline
    XLSR                            & full finetuning                & 6.5                   & 34.1                     \\ \hline
    XLSR                            & 3-steps finetuning             & -                     & 30.0                     \\ \hline
    XLS-R                           & full finetuning                & 6.1                   & 32.6                     \\ \hline
    XLS-R                           & 3-steps finetuning             & -                     & \textbf{29.2}            \\ \hline
    \multicolumn{4}{l}{\footnotesize *After post-processing with kanji-to-kana conversion.} \\
    \end{tabular}
    \label{tab:asr_comparison}
\end{table}

\section{Current Comparative Analysis of Japanese ASR Models}
A recent comparative study by \textcite{hono2024integratingpretrainedspeechlanguage} proposed an end-to-end Japanese ASR framework that integrates a pre-trained speech encoder (HuBERT) with a decoder-only large language model (GPT-NeoX) via a bridge network, aiming to avoid explicit LM-fusion and keep decoding simple. The evaluation compared the proposed model against publicly available baselines across several Japanese corpora. For efficiency, the authors reported that applying DeepSpeed-Inference improved inference speed by about $1.4\times$ while keeping recognition accuracy unchanged. In terms of CER, the proposed model outperformed reazonspeech-espnet-v1 across all beam settings on the ReazonSpeech test set (in-domain), while showing mixed outcomes on JSUT and CV8.0 compared to larger-beam decoding. 

\begin{table}[H]
\centering
\caption{ASR results reported by \textcite{hono2024integratingpretrainedspeechlanguage} on ReazonSpeech, JSUT, CV8.0, and CSJ eval sets (beam size 1 = greedy decoding).}
\label{tab:hono_asr_comparison}
{\small
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l|r|r|c|c|c|c|c|c}
\hline
\textbf{Model} & \textbf{Params} & \textbf{Beam} & \textbf{Reazon} & \textbf{JSUT} & \textbf{CV8.0} & \textbf{CSJ E3} \\
\hline
reazonspeech-espnet-v1 & 90M   & 1  & 12.0 & 8.7 & 10.5 &  15.3 \\
reazonspeech-espnet-v1 & 90M   & 20 & 8.7  & 7.5 & 8.9  &  15.5 \\
Whisper-large-v3       & 1,541M& 1  & 12.4 & 7.1 & 8.2  &  14.8 \\
Proposed (w/o DS-Inf.) & 3,708M& 1  & 8.4  & 8.6 & 9.1  &  22.9 \\
Proposed (w/ DS-Inf.)  & 3,708M& 1  & 8.4  & 8.6 & 9.2  &  22.9 \\
\hline
\end{tabular}
}
\end{table}


Current comparative analysis from these studies shows that several challenges need to be addressed. A study to compare the existing models in Japanese ASR is needed because of the unique characteristics of the Japanese language. The comparative analysis from \textcite{Karita2021} and \textcite{takahashi2024comparison} shows that while models like Conformer and Whisper have made significant strides in ASR, they still face challenges in handling the complexities of Japanese. Additionally, there is still lack of study that comparing the model from different model families because most of the existing comparative analysis only focusing on models from the same family.


\section{Datasets and Tools}
\subsection{Datasets}
Dataset is a crucial component in training and evaluating ASR models. In this study, a curated dataset collected from Japanese TEDx talks on YouTube with manual Japanese subtitles is used. TEDx talks is a rich source of diverse speech data, covering a wide range of topics and speakers, making it suitable for training ASR models. The talks are delivered by various speakers with different accents, speaking styles, and speech rates, which helps in creating a comprehensive dataset that captures the variability in real-world Japanese speech \parencite{ando2021}.

\subsection{Python}
Python was used as the main programming language to implement the data pipeline, training utilities, and evaluation scripts. It supported preprocessing tasks such as transcript cleaning, manifest generation, and training split preparation \parencite{python3.14documentation_2025}. In addition, Python libraries were used to support ASR-related workflows such as audio processing, metadata handling, diarization-based filtering, and evaluation.

\subsection{yt-dlp}
Yt-dlp is a command-line tool and Python library that allows downloading videos, audio, and subtitle tracks from YouTube. In this study, yt-dlp was used to query YouTube metadata to verify the availability of manual Japanese subtitles before downloading. Then, audio and subtitle files were downloaded for selected Japanese TEDx talks to form a consistent dataset for ASR training and evaluation \parencite{yt-dlp}.

\subsection{Audio Processing Tools}
To standardize the dataset and ensure compatibility across model families, audio was converted into a consistent format (16\,kHz, mono, 16-bit PCM). Audio resampling and waveform writing were performed using Python audio libraries \texttt{librosa} and \texttt{soundfile} \parencite{mcfee_2025_15006942,soundfile}. In addition, WebRTC voice activity detection was used to segment long TEDx recordings into shorter utterances suitable for training and evaluation \parencite{webrtc}.

\subsection{Automatic Audio Filtering Tools}
A filtering step was applied to remove non-speaker or low-quality segments such as applause and laughter. Speaker structure filtering used \texttt{pyannote.audio} diarization to prefer clips with a dominant speaker. Acoustic event filtering used a Hugging Face zero-shot audio classification pipeline with the CLAP model (\texttt{laion/clap-htsat-fused}) to identify and remove segments with high confidence of applause or laughter \parencite{clap}.

\subsection{Kaldi}
Kaldi is an open-source toolkit for speech recognition that provides a complete pipeline for feature extraction, acoustic modeling, and decoding \parencite{kaldi2011}. In this study, Kaldi was used to implement the traditional GMM--HMM baseline system. The workflow included MFCC feature extraction with CMVN normalization, staged acoustic model training (mono, tri1, tri2, tri3), alignment between stages, and decoding graph generation for evaluation.

\subsection{KenLM}
KenLM was used to build an external $n$-gram language model to support decoding in the Kaldi GMM--HMM systems. In this work, \texttt{lmplz} was used to produce an ARPA-format language model, which was then converted into decoding resources within the Kaldi graph construction pipeline \parencite{kenlm}.

\subsection{Lexicon Generation Tools}
A grapheme-based lexicon was generated using a custom Python script. The script extracted vocabulary items from transcripts and converted Japanese tokens into Hepburn romanisation using \texttt{pykakasi}. The romanised output was normalized and mapped into character-level pronunciation units used to build Kaldi language resources \parencite{pykakasi}.

\subsection{SpeechBrain}
SpeechBrain is an open-source toolkit for speech processing that supports end-to-end ASR training and evaluation \parencite{ravanelli2021speechbraingeneralpurposespeechtoolkit}. In this study, SpeechBrain was used to implement the CRDNN--CTC model. It used the prepared CSV manifests and a character-level vocabulary, and training checkpoints were managed using SpeechBrain's checkpointer mechanism \parencite{speechbrain_v1}.

\subsection{Hugging Face}
Hugging Face was used to access pretrained Whisper models and supporting utilities for fine-tuning. The Transformers library was used to load Whisper checkpoints, configure Japanese decoding prompts, and generate hypotheses using \texttt{model.generate()}. Hugging Face Datasets was used to load train/validation/test CSV splits efficiently and support reproducible fine-tuning experiments \parencite{wolf2020huggingfacestransformersstateoftheartnatural}.

\subsection{Evaluation Tools}
Model evaluation used character error rate (CER), word error rate (WER), and real-time factor (RTF). CER and WER were computed using normalized edit distance between hypothesis and reference transcripts. In this study, the \texttt{jiwer} Python library was used to compute WER and to support consistent error-rate scoring across systems. RTF was computed by dividing total decoding wall time by total audio duration to measure decoding efficiency and latency behaviour \parencite{jiwer_2025}.

\section{Gaps in Literature}
Based on the literature review, shows that there is several gaps in the research on Japanese ASR. First, while there has been significant progress in ASR models, there is still a lack of comprehensive comparative studies that evaluate different model architectures specifically for Japanese. Most existing studies focus on individual models or specific families of models, making it difficult to draw broad conclusions about the best approaches for Japanese ASR. Second, many studies are using datasets that already being throughly studied, such as CSJ, which has been cleaned and preprocessed extensively. There is a need for more research using diverse and real-world datasets, such as TEDx talks, to better understand model performance in practical scenarios.

\section{Conclusion}
This chapter has reviewed the existing literature on Japanese ASR, covering traditional models like GMM--HMM and modern deep learning approaches including CRDNN-CTC, and Transformer-based models like Whisper. The unique challenges posed by the Japanese language, such as its complex writing system and character pronunciations, were discussed. Tools and datasets commonly used in Japanese ASR research were also highlighted and gaps in the literature were also being identified.