\chapter{INTRODUCTION}
\label{ch:intro}

\section{Research Background}
\label{sec:intro-bg}

% The way people interact with computers has changed rapidly throughout history. Initially, computer instructions were provided through punched cards. Nowadays, giving instruction to computers is simply by using voices. This advancement is made possible by using speech-to-text technology that converting the spoken language into text format \parencite{Xu}. Speech-to-text technologies has already existed in industries such as customer service and medicine. But with the advancement of the Machine Learning (ML) and artificial intelligence (AI), it has made the speech-to-text technology to become more precise and faster \parencite{latif2020}. These advancements have enabled its application across many areas, including transcription services and the development of inclusive tools for individuals with disabilities \parencite{Koenecke2020}.

% Although the speech-to-text technology has advance rapidly, it still has challenge like accurately transcribing Japanese language. According to \textcite{Kanno} in "An Introduction to Japanese Linguistics", Japanese language has may words that sound the same but have different meaning and to know which word is being used is based on the current context of the sentence. This is because Japanese language is using syllable-based word formation rather than individual phonemes, it means that the words are created using syllables like "ka", "ki' or "ku" instead of using a single consonant or vowel. Japanese language also using combination of three script with each has its own set of rule makes it harder to convert from spoken language to text. 

% With the advancement of machine learning and artificial intelligence, it has significantly improved speech recognition algorithms, enabling them to adapt to language nuances \parencite{xu2023recent}. This study investigates prominent models that is Whisper from Open AI, wav2vec2 by Facebook, and ChirpV2 an Universal speech model from Google. These models have their pros and cons when transcribing spoken Japanese into text and typically use learning frameworks and undergo training on extensive datasets to improve accuracy in recognizing speech patterns \parencite{ando2021}. It is important to compare Japanese speech recognition systems because most research currently focuses on English or European languages, with limited exploration of how well these systems work with Japanese, especially in casual conversations and real-world contexts.

% Only a few studies are comparing the Japanese speech recognition systems which created a gap in this area. It is important to examine how well these models can handle language feature like dialects and how well they able to transcribe spoken language based on accuracy and the speed to convert speech to text. The findings will contribute to the development of Japanese speech recognition technology.

The advancement of technology has become the driver of the importance of human-computer interaction. Human-computer interaction has been evolved from manual input into more natural interace and one of the natural interface is Automatic Speech Recognition (ASR). ASR have the capibilities to convert spoken language into text which is really useful in in application such as captioning, meeting minutes, media archiving, and voice-enabled search. In the context of japanase language, the quality of the transcription is still a challenge because of the multiple writing system that is kanji, hiragana and katakana and also context-sensitive readings that exist in the language. 


Most of the ASR pipelines is focusing on the acoustic or end-to-end model. However the real-world performance really depends on the preprocessing and feature extraction decision made before the data is fed into the model. The important preproccessing step is noise and reverberation reduction, voice activity detection (VAD), segmentation, resampling, channel and gain normalization, and cepstral mean/variance normalization (CMVN). Meanwhile the important feature extraction that need to be carried out is MFCC and PLP for traditional systems, and log-Mel filterbanks (with or without augmentation) for modern neural models. The preprocessing and feature extraction step can gave a big impact to the accuracy and stability of the model. 


In addition to the  preprocessing steps, the model's output also must be clean for easy reading, quoting, and storing. This creates additional considerations, such as post-processing and text formalization. For example, inconsistent punctuation and stopwords like "eeto" and "ano" must be handled during post-processing. This step is important to convert raw ASR output into a standardized format that can be utilized for lecture notes, reports, or subtitles.

This paper will be focusing on formal Japanese speech transcription and compare three representative modeling approaches under controlled pre-processing and feature extraction. The first model is GMM-HMM pipeline that is traditional model, the second model is CRDNN-CTC model that is hybrid model, and the last model is fine-tuned Whisper model that is transformer model. This paper will also evaluate the model performance based on Character Error Rate (CER) as the primary metric, and Word Error Rate (WER) and Real-Time Factor (RTF) as secondary metrics.

% \section{Problem Statement}

% Current speech-to-text model are trained on standardized language which might not capture the complexities of Japanese language dialect and informal expression \parencite{imaizumi2022}. This has led to the models cannot perform well when transcribing the conversational Japanese especially when informal words or dialects is being used. Despite the advancements of AI, which significantly increase the quality of text-to-speech model \parencite{Karita2021}, there is still lack of comprehensive evaluation between these models performance with Japanese language. 


% The lack of effective speech-to-text solution that tailored for Japanese language has its implication in industries. Industries that relying on speech-to-text technology such as telecommunication, education, technology, may face a problem because ineffective speech recognition can resulting in problems such as misunderstandings and will diminished the user satisfaction \parencite{Sztah√≥2023}. Additionally, the speech detection technology will not be adopted in industries if it fails to accurately capture the full spectrum of the language, limiting usability and accessibility \parencite{widyana}. Because of that, a study focused on Japanese speech recognition quality is important not only to improve practical outcome but also supports the ongoing advancement in AI field. 


\section{Problem Statement}
An accurate and efficient Japanese transcription in ASR is not solely reliant on the model, but also on the preprocessing part like noise handling, VAD/segmentation, normalization (CMVN), and the choice of features (MFCC vs. log-Mel). Previous work in this domain usually focuses on the model itself and rarely discusses the impact of preprocessing choices side by side across model families, and reporting of character error rate (CER), word error rate (WER), and real-time factor (RTF) is often inconsistent. This will cause an issue when picking the correct model pipeline that is reliable and reproducible.

The issue remains that there is still no clear and evidence-based preprocessing and feature extraction that will create lower CER and RTF for formal Japanese language across traditional, hybrid, and fine-tuned transformers model. Because of this, there is still no obvious ASR model to pick that has the best accuracy and reliability in the context of formal Japanese language. Because of that, by comparing the preprocessing pipelines, testing MFCC and log-Mel variants under the same conditions, the performance of GMM-HMM, CRDNN-CTC, and fine-tuned Whisper can be fairly compared, filling a gap in the literature and providing actionable insights for practitioners.
 
\section{Research Objectives}
\begin{enumerate}
    \item To identify the key requirements for constructing speech-to-text model within the context of Japanese language.
    
    \item To analyze speech-to-text models to determine the most effective pre-processing setup to reduce Character Error Rate (CER) and Real-Time Factor (RTF) in Japanese language processing.

    \item To evaluate the CER and the transcription latency using RTF of different speech-to-text model when transcribing Japanese formal and informal language.
\end{enumerate}

\section{Research Questions}
\begin{enumerate}
    \item What is the key requirements for constructing speech-to-text model within the context of Japanese language.
    
    \item What is the most effective pre-processing setup to reduce Character Error Rate (CER) and Real-Time Factor (RTF) in Japanese language processing.

    \item How to calculate the performance and effectiveness using CER and RTF of different speech-to-text model in context of Japanese language?
\end{enumerate}


\section{Scope of Study}
This study will be focusing on speech-to-text transcription of Japanese language using only formal speech. The dialect speech will not be included in this study. The main focus of this study is to produce a readable and standardized text rather than detect dialects or classify speaking styles. For the preproccessing and feature extraction, this study will be focusing on noise/reverberation reduction, VAD/segmentation, resampling and normalization (CMVN), and two feature families of MFCC ($\pm\Delta/\Delta\Delta$) and log-Mel (with/without SpecAugment or frame stacking).

The models that will be compared in this study are GMM-HMM, CRDNN-CTC, and fine-tuned Whisper. The evaluation metrics that will be used in this study are Character Error Rate (CER) as the primary metric, and Word Error Rate (WER) and Real-Time Factor (RTF) as secondary metrics. The RTF will be calculated by dividing the total decoding wall-clock time by the total audio duration, using a fixed hardware/software setup.For the text post-processing, this study will be focusing on normalizing the output for formal use by removing fillers like "eeto" and "ano", numeric and punctuation normalization, and consistent script conventions. Semantic editing and translation are out of scope for this study.

For the dataset, this study will only be using formal-domain Japanese speech with available transcripts. The data that will be used is JSUT corpus \parencite{jsut}. The JSUT corpus consist of 10 hours of read speech from a single female speaker, covering various topics and is designed for TTS research. 
% create red comment below% 

\textcolor{red}{*** dataset part is not final yet, need to find more formal Japanese dataset with more speakers and longer duration ***}


\section{Significance of Study}
This study is aimed to address the gap of effective speech-to-text solution that focusing on Japanese language. Most of the developed models is focusing on English language or a generic transcribe model that is developed for multi-language. Organization that rely on accurate transcript like broadcasters, government agencies, and archives rely on this kind of technology. This thesis will be a guidance to determine which pre-processing and feature configurations most improve outcomes for formal Japanese across different model types.

This study also contributes to the research by providing a systematic comparison of preprocessing and feature extraction choices across traditional, hybrid, and fine-tuned transformer models in the context of formal Japanese transcription. By filling this gap, the findings will inform best practices for ASR system design in Japanese, supporting both academic research and practical applications in industries that depend on accurate speech-to-text conversion.


\section{Conclusion}
In this chapter, the advancement in machine learning and artificial intelligence that made the computer can understand human better by improving the speech to text model accuracy and speed has been discussed. However, there is still challenges to transcribe a language that has complex structure like Japanese that include syllable-based formation and the use of multiple writing systems. Because of this, a study to find which implementation and which model is the most performance for handling Japanese language. The finding from this study is very important to answer the question of which model is the best for speech-to-text solution in Japanese language. By identifying the specific linguistic challenges and comparing these models, this study will provide a valuable information that will be able to guide future advancements in speech-to-text technology in Japanese language and ultimately will be able to support its broader application across the industries that rely heavily on precise and efficient transcription.


