\chapter{INTRODUCTION}
\label{ch:intro}

\section{Research Background}
The advancement of technology has become the driver of the importance of human-computer interaction. Human-computer interaction has been evolved from manual input into more natural interace and one of the natural interface is Automatic Speech Recognition (ASR). ASR have the capibilities to convert spoken language into text which is really useful in in application such as captioning, meeting minutes, media archiving, and voice-enabled search\parencite{Xu}. In the context of japanese language, the quality of the transcription is still a challenge because of the multiple writing system that is kanji, hiragana and katakana and also context-sensitive readings that exist in the language \parencite{Koenecke2020}.


Most of the ASR pipelines is focusing on the acoustic or end-to-end model \parencite{xu2023recent}. However the real-world performance really depends on the preprocessing and feature extraction decision made before the data is fed into the model. The important preproccessing step is noise and reverberation reduction, voice activity detection (VAD), segmentation, resampling, channel and gain normalization, and cepstral mean/variance normalization (CMVN). Meanwhile the important feature extraction that need to be carried out is MFCC and PLP for traditional systems, and log-Mel filterbanks (with or without augmentation) for modern neural models. The preprocessing and feature extraction step can gave a big impact to the accuracy and stability of the model \parencite{latif2020}. 


In addition to the  preprocessing steps, the model's output also must be clean for easy reading, quoting, and storing. This creates additional considerations, such as post-processing and text formalization. For example, inconsistent punctuation and stopwords like "eeto" and "ano" must be handled during post-processing. This step is important to convert raw ASR output into a standardized format that can be utilized for lecture notes, reports, or subtitles \parencite{ando2021}.

This paper will be focusing on formal Japanese speech transcription and compare three representative modeling approaches under controlled pre-processing and feature extraction. The first model is GMM-HMM pipeline that is traditional model, the second model is CRDNN-CTC model that is hybrid model, and the last model is fine-tuned Whisper model that is transformer model. This paper will also evaluate the model performance based on Character Error Rate (CER) as the primary metric, and Word Error Rate (WER) and Real-Time Factor (RTF) as secondary metrics.


\section{Problem Statement}
Despite rapid progress in automatic speech recognition (ASR), reliable transcription of Japanese remains difficult due to its mixed writing systems (kanji, hiragana, katakana) and context-sensitive readings, which challenge both tokenization and error scoring. At the same time, much prior work emphasizes acoustic/end-to-end architectures, while real-world accuracy and stability also hinge on upstream choices—noise handling, VAD/segmentation, resampling/normalization, and feature design (e.g., MFCC vs. log-Mel)—that are often configured inconsistently across studies. These inconsistencies, together with heterogeneous datasets and scoring conventions, obscure which model family and preprocessing pipeline actually minimize character errors for formal Japanese speech, and how accuracy trades off with practical decoding latency. As a result, there is still a documented gap in adapting and evaluating state-of-the-art systems for Japanese-specific contexts with attention to both 

This study addresses that gap by conducting a controlled, apples-to-apples comparison of three representative approaches—Kaldi-style GMM–HMM, an end-to-end CRDNN–CTC model, and a fine-tuned Whisper transformer—under a single data pipeline and a unified evaluation protocol on formal Japanese speech. All systems are trained and scored on the same held-out test set using identical, normalized reference transcripts; Character Error Rate (primary), Word Error Rate (supporting, with shared tokenization), and Real-Time Factor (latency) are computed in a consistent way to quantify both accuracy and usability, including the expected latency trade-offs of autoregressive transformers. The core problem, therefore, is to determine which combination of preprocessing/feature configuration and model family yields the lowest CER while maintaining acceptable RTF for formal Japanese transcription under matched conditions.
 
\section{Research Objectives}
\begin{enumerate}
    \item To identify the key requirements for constructing speech-to-text model within the context of Japanese language.
    
    \item To analyze speech-to-text models to determine the most effective pre-processing setup to reduce CER and RTF in Japanese language processing.

    \item To evaluate the CER and the transcription latency using RTF of different speech-to-text model when transcribing Japanese formal and informal language.
\end{enumerate}

\section{Research Questions}
\begin{enumerate}
    \item What is the key requirements for constructing speech-to-text model within the context of Japanese language.
    
    \item What is the most effective pre-processing setup to reduce CER and RTF in Japanese language processing.

    \item How to calculate the performance and effectiveness using CER and RTF of different speech-to-text model in context of Japanese language?
\end{enumerate}


\section{Scope of Study}
This study will be focusing on speech-to-text transcription of Japanese language using only formal speech. The dialect speech will not be included in this study. The main focus of this study is to produce a readable and standardized text rather than detect dialects or classify speaking styles. For the preproccessing and feature extraction, this study will be focusing on segmentation, resampling and normalization (CMVN), and two feature families of MFCC and log-Mel filterbanks.
    
The models that will be compared in this study are GMM-HMM, CRDNN-CTC, and fine-tuned Whisper. The evaluation metrics that will be used in this study are Character Error Rate (CER) as the primary metric, and Word Error Rate (WER) and Real-Time Factor (RTF) as secondary metrics. The RTF will be calculated by dividing the total decoding wall-clock time by the total audio duration, using a fixed hardware/software setup.For the text post-processing, this study will be focusing on normalizing the output for formal use by removing fillers words, numeric and punctuation normalization, and consistent script conventions. Semantic editing and translation are out of scope for this study.

For the dataset, this study will only be using formal Japanese speech with available transcripts. The data that will be used is TEDx talks in Japanese language from YouTube that is scraped using \textit{yt-dlp} tool and then the audio is extracted using \textit{ffmpeg} tool. 

\section{Significance of Study}
This study is aimed to address the gap of effective speech-to-text solution that focusing on Japanese language. Most of the developed models is focusing on English language or a generic transcribe model that is developed for multi-language. Organization that rely on accurate transcript like broadcasters, government agencies, and archives rely on this kind of technology. This thesis will be a guidance to determine which pre-processing and feature configurations most improve outcomes for formal Japanese across different model types.

This study also contributes to the research by providing a systematic comparison of preprocessing and feature extraction choices across traditional, hybrid, and fine-tuned transformer models in the context of formal Japanese transcription. By filling this gap, the findings will inform best practices for ASR system design in Japanese, supporting both academic research and practical applications in industries that depend on accurate speech-to-text conversion.


\section{Conclusion}
In this chapter, the advancement in machine learning and artificial intelligence that made the computer can understand human better by improving the speech to text model accuracy and speed has been discussed. However, there is still challenges to transcribe a language that has complex structure like Japanese that include syllable-based formation and the use of multiple writing systems. Because of this, a study to find which implementation and which model is the most performance for handling Japanese language. The finding from this study is very important to answer the question of which model is the best for speech-to-text solution in Japanese language. By identifying the specific linguistic challenges and comparing these models, this study will provide a valuable information that will be able to guide future advancements in speech-to-text technology in Japanese language and ultimately will be able to support its broader application across the industries that rely heavily on precise and efficient transcription.


